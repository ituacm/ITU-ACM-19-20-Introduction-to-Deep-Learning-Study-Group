{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C0xQvGTdtez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "#You can add different datasets with format of csv and use it\n",
        "data_train = pd.read_csv(\"ex2data1.csv\", error_bad_lines=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZqxQ1t9eOfk",
        "colab_type": "code",
        "outputId": "7fcd01ac-e752-4378-e59f-ddd5dc61ce9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_train.head(20)\n",
        "data = data_train.to_numpy()\n",
        "\n",
        "print(type(data))\n",
        "print(data.shape)\n",
        "\n",
        "random.shuffle(data)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(100, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NurM6YWbK7Ns",
        "colab_type": "code",
        "outputId": "2b4be9f1-3d57-468f-e107-d53e7f1e8b15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "\n",
        "X_train = data[0:70, 0:2]\n",
        "X_train = X_train.reshape((2, 70))\n",
        "X_train = X_train / np.linalg.norm(X_train)\n",
        "\n",
        "Y_train = data[0:70, 2]\n",
        "Y_train = Y_train.reshape((1, 70))\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "X_test = data[70:100, 0:2]\n",
        "X_test = X_test.reshape((2, 30))\n",
        "X_test = X_test / np.linalg.norm(X_test)\n",
        "\n",
        "Y_test = data[70:100, 2]\n",
        "Y_test = Y_test.reshape((1, 30))\n",
        "\n",
        "Y_train.reshape(70, 1)\n",
        "Y_test.reshape(30, 1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 70)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ-yN-atI4Y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  \n",
        "  #calculate the sigmoid function\n",
        "  a = 1/(1+np.exp(-z))\n",
        "  \n",
        "  return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgexQtjHJL49",
        "colab_type": "code",
        "outputId": "a61f8b70-50b6-486b-cc9a-d44ec4c6e6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sigmoid(0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSoiYKmFJO2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(dim):\n",
        "  \n",
        "  W = np.zeros((dim, 1))\n",
        "  b = 0\n",
        "  \n",
        "  return W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jymwbs_fMhmO",
        "colab_type": "code",
        "outputId": "f3bc3248-585f-4b54-d38a-b03e0b7252d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "W, b = initialize_parameters(2)\n",
        "print(W)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.]\n",
            " [0.]]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BHSJW1cMn2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def propagate(X, Y, W, b):\n",
        " \n",
        "  m = X_train.shape[1]\n",
        "  #forward_pass\n",
        "  Z = np.dot(W.T, X)+b\n",
        "  A = sigmoid(Z)\n",
        "\n",
        "  #compute_loss function\n",
        "  cost = -(1/m)*np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
        "  \n",
        "  #update parameters\n",
        "  dW = (1/m)*np.dot(X,(A-Y).T)\n",
        "  db = (1/m)*np.sum(A-Y)\n",
        "  \n",
        "  return dW, db, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jWH884gPBic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize(W, b, X, Y, learning_rate, iter):\n",
        "  \n",
        "  \n",
        "  costs= []\n",
        "  \n",
        "  for it in range(iter):\n",
        "    dW, db, cost = propagate(X, Y, W, b)\n",
        "    \n",
        "    W = W - learning_rate*dW\n",
        "    \n",
        "    b = b - learning_rate*db\n",
        "    if it % 100 == 0:\n",
        "      print(\"Cost in %i th iteration %f\"%(it, cost))   \n",
        "    costs.append(cost)\n",
        "    \n",
        "  parameters = {'W' : W, 'b' : b}\n",
        "  gradients = {'dW' : dW, 'db' : db}\n",
        "  \n",
        "  return parameters, gradients, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdp1eRAvRSv7",
        "colab_type": "code",
        "outputId": "a3548eaf-b54a-4975-de62-158965272988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "def predict(W, b, X):\n",
        "  A = sigmoid(np.dot(W.T, X) + b)\n",
        "  Y_hat = np.zeros((1, X.shape[1]))\n",
        "  print(\"A: \"str(A))\n",
        "  for i in range(X.shape[1]):\n",
        "    if A[0][i] > 0.5:\n",
        "      Y_hat[0][i] = True\n",
        "    else:\n",
        "      Y_hat[0][i] = False\n",
        "  \n",
        "  return Y_hat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-50-404fff57f8de>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print(\"A: \"str(A))\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELPf2QWCSdGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X_train, X_test, Y_train, Y_test, dim, learning_rate, iteration):\n",
        "  \n",
        "  W, b = initialize_parameters(dim)\n",
        "  \n",
        "  parameters, gradients, costs = optimize(W, b, X_train, Y_train, learning_rate, iteration)\n",
        "  \n",
        "  W_final = parameters['W']\n",
        "  b_final = parameters['b']\n",
        "  \n",
        "  Y_prediction_train = predict(W_final, b_final, X_train)\n",
        "  Y_prediction_test = predict(W_final, b_final, X_test)\n",
        "  \n",
        "  print(Y_prediction_test)\n",
        "  print(Y_test)\n",
        "\n",
        "  print(\"train accuracy: {} %\".format(100-np.mean(np.abs(Y_prediction_train - Y_train))*100))\n",
        "  print(\"test accuracy: {} %\".format(100-np.mean(np.abs(Y_prediction_test - Y_test))*100))\n",
        "  \n",
        "  return costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xib-GvoX6jG_",
        "colab_type": "code",
        "outputId": "88ff2ee1-c1c9-4dbd-d1a0-127258aa5661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "costs = model(X_train, X_test, Y_train, Y_test, 2, 0.001, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost in 0 th iteration 0.693147\n",
            "Cost in 100 th iteration 0.693067\n",
            "Cost in 200 th iteration 0.692990\n",
            "Cost in 300 th iteration 0.692917\n",
            "Cost in 400 th iteration 0.692848\n",
            "Cost in 500 th iteration 0.692782\n",
            "Cost in 600 th iteration 0.692720\n",
            "Cost in 700 th iteration 0.692660\n",
            "Cost in 800 th iteration 0.692604\n",
            "Cost in 900 th iteration 0.692550\n",
            "Cost in 1000 th iteration 0.692499\n",
            "Cost in 1100 th iteration 0.692451\n",
            "Cost in 1200 th iteration 0.692404\n",
            "Cost in 1300 th iteration 0.692361\n",
            "Cost in 1400 th iteration 0.692319\n",
            "Cost in 1500 th iteration 0.692279\n",
            "Cost in 1600 th iteration 0.692242\n",
            "Cost in 1700 th iteration 0.692206\n",
            "Cost in 1800 th iteration 0.692172\n",
            "Cost in 1900 th iteration 0.692139\n",
            "Cost in 2000 th iteration 0.692109\n",
            "Cost in 2100 th iteration 0.692079\n",
            "Cost in 2200 th iteration 0.692052\n",
            "Cost in 2300 th iteration 0.692025\n",
            "Cost in 2400 th iteration 0.692000\n",
            "Cost in 2500 th iteration 0.691976\n",
            "Cost in 2600 th iteration 0.691953\n",
            "Cost in 2700 th iteration 0.691932\n",
            "Cost in 2800 th iteration 0.691911\n",
            "Cost in 2900 th iteration 0.691892\n",
            "Cost in 3000 th iteration 0.691873\n",
            "Cost in 3100 th iteration 0.691856\n",
            "Cost in 3200 th iteration 0.691839\n",
            "Cost in 3300 th iteration 0.691823\n",
            "Cost in 3400 th iteration 0.691808\n",
            "Cost in 3500 th iteration 0.691793\n",
            "Cost in 3600 th iteration 0.691780\n",
            "Cost in 3700 th iteration 0.691767\n",
            "Cost in 3800 th iteration 0.691754\n",
            "Cost in 3900 th iteration 0.691742\n",
            "Cost in 4000 th iteration 0.691731\n",
            "Cost in 4100 th iteration 0.691721\n",
            "Cost in 4200 th iteration 0.691711\n",
            "Cost in 4300 th iteration 0.691701\n",
            "Cost in 4400 th iteration 0.691692\n",
            "Cost in 4500 th iteration 0.691683\n",
            "Cost in 4600 th iteration 0.691675\n",
            "Cost in 4700 th iteration 0.691667\n",
            "Cost in 4800 th iteration 0.691659\n",
            "Cost in 4900 th iteration 0.691652\n",
            "Cost in 5000 th iteration 0.691646\n",
            "Cost in 5100 th iteration 0.691639\n",
            "Cost in 5200 th iteration 0.691633\n",
            "Cost in 5300 th iteration 0.691627\n",
            "Cost in 5400 th iteration 0.691622\n",
            "Cost in 5500 th iteration 0.691617\n",
            "Cost in 5600 th iteration 0.691612\n",
            "Cost in 5700 th iteration 0.691607\n",
            "Cost in 5800 th iteration 0.691602\n",
            "Cost in 5900 th iteration 0.691598\n",
            "Cost in 6000 th iteration 0.691594\n",
            "Cost in 6100 th iteration 0.691590\n",
            "Cost in 6200 th iteration 0.691586\n",
            "Cost in 6300 th iteration 0.691583\n",
            "Cost in 6400 th iteration 0.691580\n",
            "Cost in 6500 th iteration 0.691576\n",
            "Cost in 6600 th iteration 0.691573\n",
            "Cost in 6700 th iteration 0.691570\n",
            "Cost in 6800 th iteration 0.691568\n",
            "Cost in 6900 th iteration 0.691565\n",
            "Cost in 7000 th iteration 0.691563\n",
            "Cost in 7100 th iteration 0.691560\n",
            "Cost in 7200 th iteration 0.691558\n",
            "Cost in 7300 th iteration 0.691556\n",
            "Cost in 7400 th iteration 0.691554\n",
            "Cost in 7500 th iteration 0.691552\n",
            "Cost in 7600 th iteration 0.691550\n",
            "Cost in 7700 th iteration 0.691549\n",
            "Cost in 7800 th iteration 0.691547\n",
            "Cost in 7900 th iteration 0.691545\n",
            "Cost in 8000 th iteration 0.691544\n",
            "Cost in 8100 th iteration 0.691542\n",
            "Cost in 8200 th iteration 0.691541\n",
            "Cost in 8300 th iteration 0.691540\n",
            "Cost in 8400 th iteration 0.691539\n",
            "Cost in 8500 th iteration 0.691537\n",
            "Cost in 8600 th iteration 0.691536\n",
            "Cost in 8700 th iteration 0.691535\n",
            "Cost in 8800 th iteration 0.691534\n",
            "Cost in 8900 th iteration 0.691533\n",
            "Cost in 9000 th iteration 0.691532\n",
            "Cost in 9100 th iteration 0.691532\n",
            "Cost in 9200 th iteration 0.691531\n",
            "Cost in 9300 th iteration 0.691530\n",
            "Cost in 9400 th iteration 0.691529\n",
            "Cost in 9500 th iteration 0.691529\n",
            "Cost in 9600 th iteration 0.691528\n",
            "Cost in 9700 th iteration 0.691527\n",
            "Cost in 9800 th iteration 0.691527\n",
            "Cost in 9900 th iteration 0.691526\n",
            "[[0.47385665 0.47370558 0.4738189  0.47373457 0.47378506 0.47380472\n",
            "  0.47378123 0.47368113 0.47376345 0.47368333 0.47383621 0.47367458\n",
            "  0.47381614 0.47368817 0.47384645 0.47366769 0.47365902 0.47368178\n",
            "  0.47369739 0.47372182 0.47365902 0.47368178 0.47377372 0.47367578\n",
            "  0.4737414  0.47376806 0.47378984 0.47376462 0.4738189  0.47373457\n",
            "  0.47381611 0.47379553 0.4737199  0.47363908 0.47381709 0.47371568\n",
            "  0.47368556 0.47363687 0.47364427 0.47382326 0.47368692 0.47377379\n",
            "  0.47370801 0.47367319 0.47365913 0.47373563 0.4736369  0.47367906\n",
            "  0.47381975 0.47367306 0.47366593 0.47366004 0.47368532 0.47370947\n",
            "  0.47374319 0.47376328 0.47370559 0.47367664 0.47382283 0.47369558\n",
            "  0.47365325 0.47375476 0.47362135 0.4738231  0.47367304 0.4736646\n",
            "  0.47367002 0.47372428 0.47374127 0.47378425]]\n",
            "[[0.47355597 0.47359165 0.47355332 0.47359267 0.47366554 0.47349428\n",
            "  0.47376716 0.47339853 0.47354563 0.47367626 0.47353714 0.47345608\n",
            "  0.47348029 0.47377155 0.47364569 0.47341387 0.47362621 0.47356113\n",
            "  0.47355485 0.4735492  0.47353523 0.47372906 0.47362275 0.47347368\n",
            "  0.47357209 0.47367805 0.47349169 0.47372128 0.47371678 0.47364708]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0.]]\n",
            "[[1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
            "  1. 1. 1. 0. 1. 1.]]\n",
            "train accuracy: 52.85714285714286 %\n",
            "test accuracy: 46.666666666666664 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjUfyLeJ65IS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}